\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\citation{simonyan2014,srivastava2015tvdn}
\citation{scellier17}
\citation{watts98}
\citation{humphries2008}
\citation{bartunov2018}
\citation{scellier17}
\citation{bullmore2009}
\citation{he2015,srivastava2015}
\citation{xiaohu2011,krishnan2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{scellier17}
\citation{scellier17}
\citation{hopfield1984}
\citation{bengio2015}
\citation{scellier17}
\citation{glorot2010}
\citation{schmidhuber2015}
\citation{ioffe2015}
\citation{glorot2010}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\citation{scellier17}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Theory}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Equilibrium propagation}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:eqp_formulation}{{2.1}{2}{Equilibrium propagation}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Vanishing gradient problem}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec:vangrad}{{2.2}{2}{Vanishing gradient problem}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation}{2}{section.3}\protected@file@percent }
\citation{scellier17}
\citation{glorot2010}
\citation{scellier17}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Topology of the layered network tested in \citep  {scellier17}. All pairs of neurons in adjacent layers are connected, and there are no additional connections. All connections are bidirectional. The learning rate for weights is reduced by a factor of 4 each time distance from the output decreases by one layer, to compensate for the vanishing gradient problem.\relax }}{3}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:top_basic}{{1}{3}{Topology of the layered network tested in \cite {scellier17}. All pairs of neurons in adjacent layers are connected, and there are no additional connections. All connections are bidirectional. The learning rate for weights is reduced by a factor of 4 each time distance from the output decreases by one layer, to compensate for the vanishing gradient problem.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Changes to the layered topology to compensate for the vanishing gradient problem while using a single learning rate for all weights. Red dotted lines denote connections that have been removed and black dotted lines denote their replacements. Green solid lines denote added intralayer connections. In this illustration connections have been replaced by a random layer-skipping connection with probability $p\approx 8\%$. All connections are bidirectional.\relax }}{3}{figure.caption.6}\protected@file@percent }
\newlabel{fig:top_sw}{{2}{3}{Changes to the layered topology to compensate for the vanishing gradient problem while using a single learning rate for all weights. Red dotted lines denote connections that have been removed and black dotted lines denote their replacements. Green solid lines denote added intralayer connections. In this illustration connections have been replaced by a random layer-skipping connection with probability $p\approx 8\%$. All connections are bidirectional.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Layered topology with unique learning rates}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:basic_topology}{{3.1}{3}{Layered topology with unique learning rates}{subsection.3.1}{}}
\newlabel{eqn:gb_init}{{1}{3}{Layered topology with unique learning rates}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Layered topology with single learning rate}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:basic_topology_uniform}{{3.2}{3}{Layered topology with single learning rate}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Our topology}{3}{subsection.3.3}\protected@file@percent }
\newlabel{sec:our_topology}{{3.3}{3}{Our topology}{subsection.3.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Algorithm to produce our topology\relax }}{3}{algocf.1}\protected@file@percent }
\newlabel{alg:ourtop}{{1}{3}{Our topology}{algocf.1}{}}
\citation{scellier17}
\citation{scellier17}
\citation{mnist1998}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Tracking the training rates of individual pairs of layers}{4}{subsection.3.4}\protected@file@percent }
\newlabel{eqn:rms_correction}{{2}{4}{Tracking the training rates of individual pairs of layers}{equation.3.2}{}}
\newlabel{eqn:running_avg}{{3}{4}{Tracking the training rates of individual pairs of layers}{equation.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of performance of network topologies on MNIST dataset. Shown are test error (dashed lines) and training error (solid lines). In green is a network with the layered topology and a single learning rate (section \ref  {sec:basic_topology_uniform}). In orange is a network with the layered topology and unique learning rates (section \ref  {sec:basic_topology}), tuned to counter the vanishing gradient problem; this is a recreation of the 5-layer network in \citep  {scellier17}. In black is a network with our topology, $p=7.56\%$ (section \ref  {sec:our_topology}). The network with a layered topology and a single learning rate performs poorly because it suffers from the vanishing gradient problem. The problem can be solved by introducing unique learning rates, or by implementing our topology.\relax }}{4}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mnist_comparison}{{3}{4}{Comparison of performance of network topologies on MNIST dataset. Shown are test error (dashed lines) and training error (solid lines). In green is a network with the layered topology and a single learning rate (section \ref {sec:basic_topology_uniform}). In orange is a network with the layered topology and unique learning rates (section \ref {sec:basic_topology}), tuned to counter the vanishing gradient problem; this is a recreation of the 5-layer network in \cite {scellier17}. In black is a network with our topology, $p=7.56\%$ (section \ref {sec:our_topology}). The network with a layered topology and a single learning rate performs poorly because it suffers from the vanishing gradient problem. The problem can be solved by introducing unique learning rates, or by implementing our topology.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Network performance comparison}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:network_performance}{{4.1}{4}{Network performance comparison}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training rates of individual pairs of layers}{4}{subsection.4.2}\protected@file@percent }
\newlabel{sec:mnist_perlayer}{{4.2}{4}{Training rates of individual pairs of layers}{subsection.4.2}{}}
\citation{lee2015,xie2003,pineda1987}
\citation{lillicrap2014}
\citation{@crafton2019}
\citation{oconnor2018}
\citation{bengio2015}
\citation{bartunov2018}
\citation{shainline2019,davies2018,nahmias2013}
\citation{he2015,srivastava2015}
\citation{xiaohu2011,krishnan2019}
\citation{ioffe2015,glorot2010}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Extent of the vanishing gradient problem in different network topologies while training on MNIST dataset. (left) A network with the layered topology and a single learning rate. The training rate of a layer decreases by a factor of $\sim 4$ with each additional layer. (center) A network with the layered topology and unique per-layer learning rates. The vanishing gradient is counteracted by hand-tuning a learning rate hyperparameter for each layer. (right) A network with our topology, $p = 7.56\%$. The new topology eliminates the need for hand-tuning per-layer learning rate hyperparameters.\relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig:mnist_layers}{{4}{5}{Extent of the vanishing gradient problem in different network topologies while training on MNIST dataset. (left) A network with the layered topology and a single learning rate. The training rate of a layer decreases by a factor of $\sim 4$ with each additional layer. (center) A network with the layered topology and unique per-layer learning rates. The vanishing gradient is counteracted by hand-tuning a learning rate hyperparameter for each layer. (right) A network with our topology, $p = 7.56\%$. The new topology eliminates the need for hand-tuning per-layer learning rate hyperparameters.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Error rate after one epoch as connections are added}{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:mnist_1epoch}{{4.3}{5}{Error rate after one epoch as connections are added}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance of a network with varying $p$ during the initial stages of training. (top) The training error after one epoch. (bottom) Training rate for each layer after one epoch. There is little improvement for $p<.1\%$, after which the error rate decreases at a rate that is slightly slower than exponential with $p$. Improvement begins after the training rates of non-output layers converge, and improvement beyond that point is associated with convergence between the rates of the output layer and the non-output layers.\relax }}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:mnist_1epoch}{{5}{5}{Performance of a network with varying $p$ during the initial stages of training. (top) The training error after one epoch. (bottom) Training rate for each layer after one epoch. There is little improvement for $p<.1\%$, after which the error rate decreases at a rate that is slightly slower than exponential with $p$. Improvement begins after the training rates of non-output layers converge, and improvement beyond that point is associated with convergence between the rates of the output layer and the non-output layers.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Related work}{5}{section.5}\protected@file@percent }
\citation{he2015,ioffe2015}
\bibstyle{plain}
\bibdata{references}
\bibcite{bartunov2018}{{1}{}{{}}{{}}}
\bibcite{bengio2015}{{2}{}{{}}{{}}}
\bibcite{bullmore2009}{{3}{}{{}}{{}}}
\bibcite{@crafton2019}{{4}{}{{}}{{}}}
\bibcite{davies2018}{{5}{}{{}}{{}}}
\bibcite{glorot2010}{{6}{}{{}}{{}}}
\bibcite{he2015}{{7}{}{{}}{{}}}
\bibcite{hopfield1984}{{8}{}{{}}{{}}}
\bibcite{humphries2008}{{9}{}{{}}{{}}}
\bibcite{ioffe2015}{{10}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion and Future Research}{6}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Characteristics of an effective network}{6}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Factors influencing network performance}{6}{subsection.6.2}\protected@file@percent }
\newlabel{sec:disc_factors}{{6.2}{6}{Factors influencing network performance}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Directions for Future Research}{6}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{6}{section*.12}\protected@file@percent }
\bibcite{krishnan2019}{{11}{}{{}}{{}}}
\bibcite{mnist1998}{{12}{}{{}}{{}}}
\bibcite{lee2015}{{13}{}{{}}{{}}}
\bibcite{lillicrap2014}{{14}{}{{}}{{}}}
\bibcite{nahmias2013}{{15}{}{{}}{{}}}
\bibcite{oconnor2018}{{16}{}{{}}{{}}}
\bibcite{pineda1987}{{17}{}{{}}{{}}}
\bibcite{scellier17}{{18}{}{{}}{{}}}
\bibcite{schmidhuber2015}{{19}{}{{}}{{}}}
\bibcite{shainline2019}{{20}{}{{}}{{}}}
\bibcite{simonyan2014}{{21}{}{{}}{{}}}
\bibcite{srivastava2015tvdn}{{22}{}{{}}{{}}}
\bibcite{srivastava2015}{{23}{}{{}}{{}}}
\bibcite{watts98}{{24}{}{{}}{{}}}
\bibcite{xiaohu2011}{{25}{}{{}}{{}}}
\bibcite{xie2003}{{26}{}{{}}{{}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{6.25499pt}
\newlabel{tocindent2}{10.34999pt}
\newlabel{tocindent3}{18.198pt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{TotPages}{{7}{7}{}{page.7}{}}
